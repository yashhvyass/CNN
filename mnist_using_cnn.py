# -*- coding: utf-8 -*-
"""MNIST_using_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VKDfGKpfqH1aktQyYSxcW_Ru6QvbRgtt
"""

import torchvision
import torch
from torchvision import transforms
from torch.utils.data import DataLoader
import torch.nn as nn

img_path='./'
transform = transforms.Compose([
    transforms.ToTensor()
])

mnist_dataset = torchvision.datasets.MNIST(root=img_path, train=True, transform=transform, download=True)

from torch.utils.data import Subset
## to create validation split from the training data. Took starting 10,000 rows becasue data is randomly aranged.
mnist_valid_dataset = Subset(mnist_dataset, torch.arange(10_000))
mnist_train_dataset = Subset(mnist_dataset, torch.arange(10_000, len(mnist_dataset)))

mnist_test_dataset = torchvision.datasets.MNIST(root=img_path, train=False, transform=transform, download=True)

## batch for mini batch/sometimes called batch gradient descent.
batch_size=64
torch.manual_seed(42)

# Create a dataloader
train_dl = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)
valid_dl = DataLoader(mnist_valid_dataset, batch_size=batch_size, shuffle=False)

# Visualizing the first image.
for i, value in enumerate(train_dl):
  if i<1:
    print(f'The {i}th image value: {value}')
    print(f'\nThe size of the 1st image is: {value[0].shape}')
  else:
    break

"""1. Since the size of featuresa are between [0,1].
2. We already converted images to tensor.
3. The labels are integers from (0,9).

Therefore we don't need to do any scaling or further conversion.
"""

## Creating the model
model = nn.Sequential()
## padding=2; using same padding mode preserves the spatial dataset. To apply same padding mode check the formula to calculate output feature map.
model.add_module('conv1', nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2))
model.add_module('relu1', nn.ReLU())
model.add_module('pool1', nn.MaxPool2d(kernel_size=2)) ## if stride is not specified, it is set equal to kernel_size. This is reduce the spatial dimensions by half.
model.add_module('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2))
model.add_module('relu2', nn.ReLU())
model.add_module('pool2', nn.MaxPool2d(kernel_size=2))

# Pytorch also provides a way to calculate output feature map.
x = torch.ones((4,1,28,28))
model(x).shape

model.add_module('flatten', nn.Flatten())
x=torch.ones((4,1,28,28))
model(x).shape

model.add_module('fc1', nn.Linear(3136, 1024))
model.add_module('relu3', nn.ReLU())
## Applying Dropout between fc layers for regularization
model.add_module('fc2', nn.Linear(1024, 10)) ## don't need to add softmax because crossentropyloss function already do softmax internally.

loss_fn = nn.CrossEntropyLoss()
## The advantage of Adam is the choice of step size derived from the running average of gradient moments.
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

## Training and Validation Loop
def train(model, num_epochs, train_dl, valid_dl):
  loss_hist_train = [0]*num_epochs
  accuracy_hist_train = [0]*num_epochs
  loss_hist_valid = [0]*num_epochs
  accuracy_hist_valid = [0]*num_epochs

  # The designated settings for training model.train() and evaluation model.eval() will automatically set the mode for the dropout layer and rescale the hidde units appropriately.

  for epoch in range(num_epochs):
    model.train()
    for x_batch, y_batch in train_dl:
      pred = model(x_batch)
      loss = loss_fn(pred, y_batch)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()

      loss_hist_train[epoch] += loss.item()*y_batch.size(0)
      is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
      accuracy_hist_train[epoch] += is_correct.sum()

    loss_hist_train[epoch] /= len(train_dl.dataset)
    accuracy_hist_train[epoch] /= len(train_dl.dataset)

    model.eval()
    with torch.no_grad():
      for x_batch, y_batch in valid_dl:
        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        loss_hist_valid[epoch] += loss.item()*y_batch.size(0)
        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
        accuracy_hist_valid[epoch] += is_correct.sum()

    loss_hist_valid[epoch] /= len(valid_dl.dataset)
    accuracy_hist_valid[epoch] /= len(valid_dl.dataset)

    if epoch%2 == 0:
      print(f'Epoch: {epoch+1}; Train Accuracy: {accuracy_hist_train[epoch]: .4f}; Val Accuracy: {accuracy_hist_valid[epoch]:.4f}')

  return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid

torch.manual_seed(42)
num_epochs=3 ## used only 3 epochs for shorter runtime. You can use larger runtime as well.

hist = train(model, num_epochs, train_dl, valid_dl)

import matplotlib.pyplot as plt
import numpy as np
x_arr = np.arange(len(hist[0])) + 1
fig = plt.figure(figsize=(12, 5))

ax = fig.add_subplot(1,2,1)
ax.plot(x_arr, hist[0], '-o', label='Train Loss')
ax.plot(x_arr, hist[1], '--<', label='Validation Loss')
ax.legend(fontsize=15)

ax = fig.add_subplot(1,2,2)
ax.plot(x_arr, hist[2], '-o', label='Train Acc')
ax.plot(x_arr, hist[3], '--<', label='Validation Acc')
ax.legend(fontsize=15)

ax.set_xlabel('Epoch', size=15)
ax.set_xlabel('Accuracy', size=15)
plt.show()

"""## Evaluate Model on the Test Dataset"""

pred = model(mnist_test_dataset.data.unsqueeze(1)/255.) ## use unsqueeze to get data into proper shape
is_correct = (torch.argmax(pred, dim=1) == mnist_test_dataset.targets).float()
print(f'Test Accuracy: {is_correct.mean():.4f}')

"""## Visualize batch data and predicted exmaples."""

fig = plt.figure(figsize=(12, 5))
for i in range(12):
  ax = fig.add_subplot(2, 6,i+1)
  ax.set_xticks([])
  ax.set_yticks([])
  img = mnist_test_dataset[i][0][0, :, :]
  pred = model(img.unsqueeze(0).unsqueeze(1))
  y_pred = torch.argmax(pred)
  ax.imshow(img, cmap='gray_r')
  ax.text(0.9, 0.1, y_pred.item(), size=15, color='blue', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)

plt.show()

